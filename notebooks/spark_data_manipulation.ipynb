{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cubic-travel",
   "metadata": {},
   "source": [
    "# spark ことはじめ\n",
    "\n",
    "## sparkについて\n",
    "spark はサイズの大きいデータを扱うための分散処理フレームワーク。  \n",
    "HadoopのようなものでHadoopはJavaであるのに対し、sparkはscalaで記述されている。\n",
    "scala, java python, RなどでAPIが利用できる。\n",
    "\n",
    "sparkはHadoop内でのMapReduceの部分にあたる。  \n",
    "Hadoopと違い、メモリ上で実行するのでHadoopよりも早い。（Prestoのようなもの？）\n",
    "\n",
    "\n",
    "## トピックス\n",
    "調べたこと\n",
    "\n",
    "### RDD(Resilient Distributed Datasets)\n",
    "RDDとは、耐障害性分散データセットで__繰り返し使用するデータをメモリ上に保持する__。  \n",
    "\n",
    "* 遅延評価（特定のメソッドが呼ばれるまで、データは処理されない）\n",
    "* 読み取り専用\n",
    "* イミュータブルである\n",
    "* 再利用のためにメモリ上にキャッシュされる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "helpful-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "tender-cyprus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.11.4:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-notebook</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x13146b250>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# セッションを開く\n",
    "try:\n",
    "    spark.stop()\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "spark = SparkSession.builder.appName(\"pyspark-notebook\").master(master=\"local[*]\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "instructional-marijuana",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version : 3.8.5 (default, Jul 21 2020, 10:48:26) \n",
      "[Clang 11.0.3 (clang-1103.0.32.62)]\n",
      "Spark version : 3.1.1\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "バージョン確認\n",
    "\"\"\"\n",
    "print (f\"Python version : {sys.version}\")\n",
    "print (f\"Spark version : {spark.version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "starting-engineer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+------+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|target|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+------+\n",
      "|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|  24.0|\n",
      "|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|  21.6|\n",
      "|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|  34.7|\n",
      "|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|  33.4|\n",
      "|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|  36.2|\n",
      "|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|  28.7|\n",
      "|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|  22.9|\n",
      "|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|  27.1|\n",
      "|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|  16.5|\n",
      "|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|  18.9|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "#bostonデータを取得し、sparkのdataframeへ変換\n",
    "boston_data = load_boston()\n",
    "\n",
    "boston_df = pd.DataFrame(boston_data[\"data\"], columns=boston_data[\"feature_names\"])\n",
    "boston_df[\"target\"] = boston_data[\"target\"]\n",
    "feature_cols = boston_data[\"feature_names\"]\n",
    "boston_df = spark.createDataFrame(boston_df)\n",
    "boston_df.show(10)\n",
    "\n",
    "print(type(boston_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-guidance",
   "metadata": {},
   "source": [
    "#### TransformerとAction\n",
    "sparkにおいてrddを直接扱うのは理想ではない。(sparkが最適化できる余地を無くすため)  \n",
    "`spark.sql.functions`を利用するのが理想。\n",
    "\n",
    "Actionは実行されるたびにRDDが計算されるので、適宜中間データを`cache` or `永続化`をした方がいい場合がある。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "amended-intranet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fundamental-technician",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              hoge|\n",
      "+------------------+\n",
      "|             28.98|\n",
      "|30.740000000000002|\n",
      "|38.730000000000004|\n",
      "|36.339999999999996|\n",
      "|             41.53|\n",
      "|             33.91|\n",
      "|             35.33|\n",
      "|             46.25|\n",
      "|             46.43|\n",
      "|              36.0|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 7.36 ms, sys: 2.39 ms, total: 9.75 ms\n",
      "Wall time: 136 ms\n",
      "+------------------+\n",
      "|              hoge|\n",
      "+------------------+\n",
      "|             28.98|\n",
      "|30.740000000000002|\n",
      "|38.730000000000004|\n",
      "|36.339999999999996|\n",
      "|             41.53|\n",
      "|             33.91|\n",
      "|             35.33|\n",
      "|             46.25|\n",
      "|             46.43|\n",
      "|              36.0|\n",
      "+------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 2.29 ms, sys: 635 µs, total: 2.92 ms\n",
      "Wall time: 37.4 ms\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+------+\n",
      "|   CRIM|  ZN|INDUS|CHAS|  NOX|   RM|  AGE|   DIS|RAD|  TAX|PTRATIO|     B|LSTAT|target|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+------+\n",
      "|0.00632|18.0| 2.31| 0.0|0.538|6.575| 65.2|  4.09|1.0|296.0|   15.3| 396.9| 4.98|  24.0|\n",
      "|0.02731| 0.0| 7.07| 0.0|0.469|6.421| 78.9|4.9671|2.0|242.0|   17.8| 396.9| 9.14|  21.6|\n",
      "|0.02729| 0.0| 7.07| 0.0|0.469|7.185| 61.1|4.9671|2.0|242.0|   17.8|392.83| 4.03|  34.7|\n",
      "|0.03237| 0.0| 2.18| 0.0|0.458|6.998| 45.8|6.0622|3.0|222.0|   18.7|394.63| 2.94|  33.4|\n",
      "|0.06905| 0.0| 2.18| 0.0|0.458|7.147| 54.2|6.0622|3.0|222.0|   18.7| 396.9| 5.33|  36.2|\n",
      "|0.02985| 0.0| 2.18| 0.0|0.458| 6.43| 58.7|6.0622|3.0|222.0|   18.7|394.12| 5.21|  28.7|\n",
      "|0.08829|12.5| 7.87| 0.0|0.524|6.012| 66.6|5.5605|5.0|311.0|   15.2| 395.6|12.43|  22.9|\n",
      "|0.14455|12.5| 7.87| 0.0|0.524|6.172| 96.1|5.9505|5.0|311.0|   15.2| 396.9|19.15|  27.1|\n",
      "|0.21124|12.5| 7.87| 0.0|0.524|5.631|100.0|6.0821|5.0|311.0|   15.2|386.63|29.93|  16.5|\n",
      "|0.17004|12.5| 7.87| 0.0|0.524|6.004| 85.9|6.5921|5.0|311.0|   15.2|386.71| 17.1|  18.9|\n",
      "+-------+----+-----+----+-----+-----+-----+------+---+-----+-------+------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### カラム同士の和\n",
    "#rddを使った演算\n",
    "%time boston_df.rdd.map(lambda x : [x.target + x.LSTAT]).toDF([\"hoge\"]).select([\"hoge\"]).show(10)\n",
    "\n",
    "#sql functionsの演算\n",
    "%time boston_df.withColumn(\"hoge\", F.col(\"target\") + F.col(\"LSTAT\")).select([\"hoge\"]).show(10)\n",
    "\n",
    "#元のテーブルを変更するわけではない\n",
    "boston_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "organizational-active",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+\n",
      "|col1|col2|      col3|\n",
      "+----+----+----------+\n",
      "| 616| mjl|[hab, ayc]|\n",
      "| 757| fcq|[jup, zn1]|\n",
      "| 483| hjf|[o9h, 11g]|\n",
      "| 855| rrm|[vr6, g7q]|\n",
      "| 060| ovq|[pex, 72f]|\n",
      "| 135| fly|[sla, 1jh]|\n",
      "| 128| ssc|[h92, 7v6]|\n",
      "| 438| dss|[yff, gnk]|\n",
      "| 857| nzp|[993, 32n]|\n",
      "| 774| lhx|[wbk, h7m]|\n",
      "+----+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "from pyspark.sql import Row\n",
    "\n",
    "digits = string.digits\n",
    "lowercase = string.ascii_lowercase\n",
    "\n",
    "np.random.choice(list(digits), 3)\n",
    "\n",
    "rows = [\n",
    "    Row(\n",
    "        col1=\"\".join(np.random.choice(list(digits), 3))\n",
    "        , col2=\"\".join(np.random.choice(list(lowercase), 3))\n",
    "        , col3=[\n",
    "            \"\".join(np.random.choice(list(digits+lowercase), 3))\n",
    "            , \"\".join(np.random.choice(list(digits+lowercase), 3))\n",
    "        ]\n",
    "    ) for _ in range(10)\n",
    "]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(rows)\n",
    "\n",
    "array_df = spark.createDataFrame(rdd)\n",
    "array_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "auburn-introduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|arr_0|arr_1|\n",
      "+-----+-----+\n",
      "|  hab|  ayc|\n",
      "|  jup|  zn1|\n",
      "|  o9h|  11g|\n",
      "|  vr6|  g7q|\n",
      "|  pex|  72f|\n",
      "|  sla|  1jh|\n",
      "|  h92|  7v6|\n",
      "|  yff|  gnk|\n",
      "|  993|  32n|\n",
      "|  wbk|  h7m|\n",
      "+-----+-----+\n",
      "\n",
      "CPU times: user 6.93 ms, sys: 2.4 ms, total: 9.32 ms\n",
      "Wall time: 136 ms\n",
      "+----+----+----------+-----+-----+\n",
      "|col1|col2|      col3|arr_0|arr_1|\n",
      "+----+----+----------+-----+-----+\n",
      "| 616| mjl|[hab, ayc]|  hab|  ayc|\n",
      "| 757| fcq|[jup, zn1]|  jup|  zn1|\n",
      "| 483| hjf|[o9h, 11g]|  o9h|  11g|\n",
      "| 855| rrm|[vr6, g7q]|  vr6|  g7q|\n",
      "| 060| ovq|[pex, 72f]|  pex|  72f|\n",
      "| 135| fly|[sla, 1jh]|  sla|  1jh|\n",
      "| 128| ssc|[h92, 7v6]|  h92|  7v6|\n",
      "| 438| dss|[yff, gnk]|  yff|  gnk|\n",
      "| 857| nzp|[993, 32n]|  993|  32n|\n",
      "| 774| lhx|[wbk, h7m]|  wbk|  h7m|\n",
      "+----+----+----------+-----+-----+\n",
      "\n",
      "CPU times: user 1.64 ms, sys: 419 µs, total: 2.06 ms\n",
      "Wall time: 59.2 ms\n"
     ]
    }
   ],
   "source": [
    "#rddを使った演算\n",
    "%time array_df.rdd.map(lambda x: [x.col3[0], x.col3[1]]).toDF([\"arr_0\", \"arr_1\"]).show()\n",
    "\n",
    "#sql functionsの演算\n",
    "%time array_df.withColumn(\"arr_0\", array_df[\"col3\"].getItem(0)).withColumn(\"arr_1\", array_df[\"col3\"].getItem(1)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_study",
   "language": "python",
   "name": "ml_study-mpxukohs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "otherwise-sense",
   "metadata": {},
   "source": [
    "## 学習用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "generous-breeding",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, f1_score, log_loss, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "needed-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_all = load_breast_cancer(as_frame=True)\n",
    "data = data_all[\"data\"]\n",
    "label = data_all[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "liquid-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(data, label, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-oakland",
   "metadata": {},
   "source": [
    "### 基底クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organic-childhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../code/models/\")\n",
    "from model import MetaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "organic-springfield",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module model:\n",
      "\n",
      "__init__(self, name: str, params: dict) -> None\n",
      "    コンストラクタ\n",
      "    \n",
      "    Parameters\n",
      "    -----\n",
      "    name : str\n",
      "        モデルを管理する識別子\n",
      "    params : dict\n",
      "        ハイパーパラメータ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MetaModel.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "macro-infection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train in module model:\n",
      "\n",
      "train(self, tr_x: pandas.core.frame.DataFrame, tr_y: pandas.core.frame.DataFrame, va_x: pandas.core.frame.DataFrame = None, va_y: pandas.core.frame.DataFrame = None)\n",
      "    モデルの学習を行う関数\n",
      "    \n",
      "    Parameters \n",
      "    -----\n",
      "    tr_x : pd.DataFrame\n",
      "        学習データの特徴量\n",
      "    tr_y : pd.DataFrame\n",
      "        学習データの目的変数\n",
      "    va_x : pd.DataFrame\n",
      "        検証データの特徴量\n",
      "    va_y : pd.DataFrame\n",
      "        検証データの目的変数\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MetaModel.train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pursuant-indie",
   "metadata": {},
   "source": [
    "### lightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sticky-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "\n",
    "from model import MetaModel\n",
    "from utils import Util\n",
    "\n",
    "class ModelLgb(MetaModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    LightGBMのラッパークラス\n",
    "\n",
    "    Attributes\n",
    "    -----\n",
    "    name : str\n",
    "        モデルの識別子\n",
    "    params : dict\n",
    "        モデルのハイパーパラメータ\n",
    "    metric_log : dict\n",
    "        学習過程を保存する辞書\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name:str, params:dict):\n",
    "        \"\"\"\n",
    "        初期化処理\n",
    "        \n",
    "        Paramters\n",
    "        -----\n",
    "        name: str\n",
    "            モデル名\n",
    "        params: dict\n",
    "            モデルの学習に用いるパラメーター\n",
    "        \"\"\"\n",
    "        super().__init__(f\"lgb-{name}\", params)\n",
    "        \n",
    "        self.metric_log = {}\n",
    "        self.model = None\n",
    "    \n",
    "    def train(self, tr_x:pd.DataFrame, tr_y:pd.DataFrame, va_x:pd.DataFrame=None, va_y:pd.DataFrame=None) -> None:\n",
    "        \"\"\"\n",
    "        モデルの学習を行う。\n",
    "        \n",
    "        Parameters\n",
    "        -----\n",
    "        tr_x, tr_y : pd.DataFrame\n",
    "            学習用データの特徴量とラベル\n",
    "        va_x, va_y : pd.DataFrame\n",
    "            検証用データの特徴量とラベル\n",
    "            \n",
    "        Note\n",
    "        -----\n",
    "        paramsからモデルのパラメータとしてpopされる\n",
    "        * num_boost_round \n",
    "        * verbose_eval \n",
    "        * early_stopping_rounds\n",
    "        * categorical_feature \n",
    "        \"\"\"\n",
    "        #初期化\n",
    "        self.metric_log = {}\n",
    "        \n",
    "        if va_x is None and va_y is None: #validationなし\n",
    "            validate = False\n",
    "        elif va_x is not None and va_y is not None: #validationあり\n",
    "            validate = True\n",
    "        else: #エラー\n",
    "            raise ValueError(\"Both va_x and va_y must be None or not None.\")\n",
    "        \n",
    "        #データの作成\n",
    "        lgb_tr = lgb.Dataset(tr_x, label=tr_y)\n",
    "        if validate:\n",
    "            lgb_va = lgb.Dataset(va_x, label=va_y)\n",
    "        \n",
    "        #=====ハイパーパラメータ\n",
    "        params = dict(self.params)\n",
    "        #train関数の引数\n",
    "        num_boost_round = params.pop(\"num_boost_round\")\n",
    "        verbose_eval = params.pop(\"verbose_eval\") if params.get(\"verbose_eval\") is not None else 10\n",
    "        early_stopping_rounds= params.pop(\"early_stopping_rounds\") if params.get(\"early_stopping_rounds\") is not None else None\n",
    "        categorical_feature = params.pop(\"categorical_feature\") if params.get(\"categorical_feature\") is not None else \"auto\"\n",
    "\n",
    "        #モデルの訓練\n",
    "        if validate:\n",
    "            valid_sets=[lgb_tr, lgb_va]\n",
    "            valid_names=['train', 'valid']\n",
    "            \n",
    "            self.model = lgb.train(\n",
    "                params\n",
    "                , train_set=lgb_tr\n",
    "                , num_boost_round=num_boost_round\n",
    "                , valid_sets=valid_sets\n",
    "                , valid_names=valid_names\n",
    "                , early_stopping_rounds=early_stopping_rounds\n",
    "                , verbose_eval=verbose_eval\n",
    "                , evals_result=self.metric_log\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            #validationデータの追加\n",
    "            valid_sets=[lgb_tr]\n",
    "            valid_names=['train']\n",
    "            \n",
    "            self.model = lgb.train(\n",
    "                params\n",
    "                , train_set=lgb_tr\n",
    "                , num_boost_round=num_boost_round\n",
    "                , valid_sets=valid_sets\n",
    "                , valid_names=valid_names\n",
    "                , verbose_eval=verbose_eval\n",
    "                , evals_result=self.metric_log\n",
    "            )\n",
    "        \n",
    "    def predict(self, test_x: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        予測を行う\n",
    "        \n",
    "        Parameters\n",
    "        -----\n",
    "        test_x : pd.DataFrame\n",
    "            予測する特徴量データ\n",
    "            \n",
    "        Returns\n",
    "        predict_val : np.ndarray\n",
    "            モデルによる予測値\n",
    "        \"\"\"\n",
    "        predict_val = self.model.predict(test_x, num_iteration=self.model.best_iteration)\n",
    "        \n",
    "        return predict_val\n",
    "    \n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"\n",
    "        モデルを保存する\n",
    "        \n",
    "        Note\n",
    "        -----\n",
    "        best_iterationを残すためにpkl形式で保存\n",
    "        \"\"\"\n",
    "        model_path = pathlib.Path(f\"../model/{self.name}.model\")\n",
    "        #pickleで保存\n",
    "        Util.dump(self, model_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, path:pathlib.Path) -> __class__:\n",
    "        \"\"\"\n",
    "        モデルを読み込む\n",
    "        \n",
    "        Parameters\n",
    "        -----\n",
    "        path : pathlib.Path or str\n",
    "            path of model file\n",
    "            \n",
    "        Returns\n",
    "        -----\n",
    "        model : ModelLgb\n",
    "            指定されたモデルファイルを読み込んだもの\n",
    "        \"\"\"\n",
    "        cls = Util.load(path)\n",
    "        return cls\n",
    "   \n",
    "    def feature_importance(self, importance_type:str=\"gain\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        モデルのfeature importanceを返却する\n",
    "\n",
    "        Parameters\n",
    "        -----\n",
    "        importance_type : str, optional(default=\"gain\")\n",
    "            importanceの種類\n",
    "        \n",
    "        Returns\n",
    "        -----\n",
    "        df_importance :pd.DataFrame\n",
    "            importanceの値\n",
    "        \"\"\"\n",
    "        feature_name = self.model.feature_name()\n",
    "        feature_importance = self.model.feature_importance(importance_type=importance_type, iteration=self.model.best_iteration)\n",
    "\n",
    "        #DataFrameに変換\n",
    "        df_importance = pd.DataFrame(\n",
    "            [feature_name, feature_importance]\n",
    "            , index=[\"feature_name\", importance_type]\n",
    "        ).T\n",
    "        \n",
    "        return df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "statutory-rental",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_lgb import ModelLgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "every-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = {\n",
    "    \"boosting_type\":\"gbdt\"\n",
    "    , \"objective\":\"binary\"\n",
    "    , \"metric\":\"binary_logloss\"\n",
    "    , \"max_depth\":4\n",
    "    , \"num_leaves\":15\n",
    "    , \"colsample_bytree\":1.0 \n",
    "    , \"feature_fraction_bynode\":1.0\n",
    "    , \"subsample\":0.9 \n",
    "    , \"min_child_weight\":1 \n",
    "    , \"lambda_l1\":0 #reg_alpha(L1)\n",
    "    , \"lambda_l2\":1 #reg_lambda(L2)\n",
    "    , \"learning_rate\":0.1\n",
    "    , \"verbosity\":-1\n",
    "    , \"verbose_eval\":0\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    \"num_boost_round\":100\n",
    "    , \"early_stopping_rounds\":5\n",
    "    , \"verbose_eval\":10\n",
    "}\n",
    "\n",
    "params = default_params.copy()\n",
    "params.update(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "upset-leader",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = ModelLgb(\"test_lgbm\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "negative-subject",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 5 rounds\n",
      "[10]\ttrain's binary_logloss: 0.238768\tvalid's binary_logloss: 0.294341\n",
      "[20]\ttrain's binary_logloss: 0.117775\tvalid's binary_logloss: 0.189338\n",
      "[30]\ttrain's binary_logloss: 0.0655705\tvalid's binary_logloss: 0.152034\n",
      "[40]\ttrain's binary_logloss: 0.0406951\tvalid's binary_logloss: 0.14537\n",
      "Early stopping, best iteration is:\n",
      "[41]\ttrain's binary_logloss: 0.0389491\tvalid's binary_logloss: 0.143528\n"
     ]
    }
   ],
   "source": [
    "lgb_clf.train(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "daily-figure",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "from model import MetaModel\n",
    "from utils import Util\n",
    "\n",
    "class ModelXgb(MetaModel):\n",
    "    \n",
    "    \"\"\"\n",
    "    XGBoostのラッパークラス\n",
    "\n",
    "    Attributes\n",
    "    -----\n",
    "    name : str\n",
    "        モデルの識別子\n",
    "    params : dict\n",
    "        モデルのハイパーパラメータ\n",
    "    metric_log : dict\n",
    "        学習過程を保存する辞書\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name:str, params:dict) -> None:\n",
    "        \"\"\"\n",
    "        初期化処理\n",
    "        \n",
    "        Paramters\n",
    "        -----\n",
    "        name: str\n",
    "            モデル名\n",
    "        params: dict\n",
    "            モデルの学習に用いるパラメーター\n",
    "        \"\"\"\n",
    "        super().__init__(f\"lgb-{name}\", params)\n",
    "        \n",
    "        self.metric_log = {}\n",
    "        self.model = None\n",
    "    \n",
    "    def train(self, tr_x:pd.DataFrame, tr_y:pd.DataFrame, va_x:pd.DataFrame=None, va_y:pd.DataFrame=None) -> None:\n",
    "        \"\"\"\n",
    "        モデルの学習を行う。\n",
    "        \n",
    "        Parameters\n",
    "        -----\n",
    "        tr_x, tr_y : pd.DataFrame\n",
    "            学習用データの特徴量とラベル\n",
    "        va_x, va_y : pd.DataFrame\n",
    "            検証用データの特徴量とラベル\n",
    "            \n",
    "        Note\n",
    "        -----\n",
    "        paramsからモデルのパラメータとしてpopされる\n",
    "        * num_boost_round \n",
    "        * verbose_eval \n",
    "        * early_stopping_rounds\n",
    "        * categorical_feature \n",
    "        \"\"\"\n",
    "        #初期化\n",
    "        self.metric_log = {}\n",
    "        \n",
    "        if va_x is None and va_y is None: #validationなし\n",
    "            validate = False\n",
    "        elif va_x is not None and va_y is not None: #validationあり\n",
    "            validate = True\n",
    "        else: #エラー\n",
    "            raise ValueError(\"Both va_x and va_y must be None or not None.\")\n",
    "        \n",
    "        #データの作成\n",
    "        dtrain = xgb.DMatrix(tr_x, label=tr_y)\n",
    "        if validate:\n",
    "            dvalid = xgb.DMatrix(va_x, label=va_y)\n",
    "        \n",
    "        #=====ハイパーパラメータ\n",
    "        params = dict(self.params)\n",
    "        #train関数の引数\n",
    "        num_boost_round = params.pop(\"num_boost_round\")\n",
    "        verbose_eval = params.pop(\"verbose_eval\") if params.get(\"verbose_eval\") is not None else 10\n",
    "        early_stopping_rounds= params.pop(\"early_stopping_rounds\") if params.get(\"early_stopping_rounds\") is not None else None\n",
    "\n",
    "        #モデルの訓練\n",
    "        if validate:\n",
    "            evals = [(dtrain, \"train\"), (dvalid, \"valid\")]\n",
    "            \n",
    "            self.model = xgb.train(\n",
    "                params\n",
    "                , dtrain\n",
    "                , num_boost_round=num_boost_round\n",
    "                , evals=evals\n",
    "                , early_stopping_rounds=early_stopping_rounds\n",
    "                , verbose_eval = verbose_eval\n",
    "                , evals_result=self.metric_log\n",
    "            )\n",
    "            \n",
    "        else:\n",
    "            evals = [(dtrain, \"train\")]\n",
    "            \n",
    "            self.model = xgb.train(\n",
    "                params\n",
    "                , dtrain\n",
    "                , num_boost_round=num_boost_round\n",
    "                , evals=evals\n",
    "                , verbose_eval = verbose_eval\n",
    "                , evals_result=self.metric_log\n",
    "            )\n",
    "        \n",
    "    def predict(self, test_x: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        予測を行う\n",
    "        \n",
    "        Parameters\n",
    "        -----\n",
    "        test_x : pd.DataFrame\n",
    "            予測する特徴量データ\n",
    "            \n",
    "        Returns\n",
    "        predict_val : np.ndarray\n",
    "            モデルによる予測値\n",
    "        \"\"\"\n",
    "        dtest = xgb.DMatrix(test_x)\n",
    "        predict_val = self.model.predict(dtest, ntree_limit=self.model.best_iteration)\n",
    "        \n",
    "        return predict_val\n",
    "    \n",
    "    def save_model(self) -> None:\n",
    "        \"\"\"\n",
    "        モデルを保存する\n",
    "        \n",
    "        Note\n",
    "        -----\n",
    "        best_iterationを残すためにpkl形式で保存\n",
    "        \"\"\"\n",
    "        model_path = pathlib.Path(f\"../model/{self.name}.model\")\n",
    "        #pickleで保存\n",
    "        Util.dump(self, model_path)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, path:pathlib.Path) -> __class__:\n",
    "        \"\"\"\n",
    "        モデルを読み込む\n",
    "        \n",
    "        Parameters\n",
    "        -----\n",
    "        path : pathlib.Path or str\n",
    "            path of model file\n",
    "            \n",
    "        Returns\n",
    "        -----\n",
    "        model : ModelXgb\n",
    "            指定されたモデルファイルを読み込んだもの\n",
    "        \"\"\"\n",
    "        cls = Util.load(path)\n",
    "        return cls\n",
    "   \n",
    "    def feature_importance(self, importance_type:str=\"gain\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        モデルのfeature importanceを返却する\n",
    "\n",
    "        Parameters\n",
    "        -----\n",
    "        importance_type : str, optional(default=\"gain\")\n",
    "            importanceの種類\n",
    "        \n",
    "        Returns\n",
    "        -----\n",
    "        df_importance :pd.DataFrame\n",
    "            importanceの値\n",
    "        \"\"\"\n",
    "        f_importance_map = defaultdict(int)\n",
    "        f_importance_map.update(self.model.get_score(importance_type=importance_type))\n",
    "\n",
    "        #DataFrameに変換\n",
    "        df_importance = pd.DataFrame(\n",
    "            [[col, f_importance_map[col]] for col in self.model.feature_names]\n",
    "            , columns=[\"feature_name\", importance_type]\n",
    "        )\n",
    "        return df_importance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "configured-stylus",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-34-1da2e2e26174>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-34-1da2e2e26174>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    , \"early_stopping_rounds\":5\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "default_params = {\n",
    "    \"boosting_type\":\"gbdt\"\n",
    "    , \"objective\":\"binary:logistic\"\n",
    "    , \"metric\":\"logloss\"\n",
    "    , \"eta\":0.1\n",
    "    , \"max_depth\":4\n",
    "    , \"min_child_weight\":1\n",
    "    , \"colsample_bytree\":1.0 \n",
    "    , \"colsample_bylevel\":0.3\n",
    "    , \"subsample\":0.9 \n",
    "    , \"gamma\":0\n",
    "    , \"lambda\":1 #reg_lambda(L2)\n",
    "    , \"alpha\":0 #reg_alpha(L1)\n",
    "    , \"verbosity\":0\n",
    "}\n",
    "\n",
    "training_params = {\n",
    "    #\"num_boost_round\":100\n",
    "    ,\"early_stopping_rounds\":5\n",
    "    , \"verbose_eval\":10\n",
    "}\n",
    "\n",
    "params = default_params.copy()\n",
    "params.update(training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "external-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = ModelXgb(\"test_xgb\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dried-identification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.61280\tvalid-logloss:0.61592\n",
      "[10]\ttrain-logloss:0.22426\tvalid-logloss:0.26849\n",
      "[20]\ttrain-logloss:0.10508\tvalid-logloss:0.17805\n",
      "[30]\ttrain-logloss:0.05847\tvalid-logloss:0.14732\n",
      "[40]\ttrain-logloss:0.03777\tvalid-logloss:0.13806\n",
      "[50]\ttrain-logloss:0.02723\tvalid-logloss:0.13571\n",
      "[51]\ttrain-logloss:0.02645\tvalid-logloss:0.13522\n"
     ]
    }
   ],
   "source": [
    "xgb_clf.train(train_x, train_y, test_x, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-meter",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_study",
   "language": "python",
   "name": "ml_study-mpxukohs"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
